{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import utils.visulaiser as visulaiser\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from rouge_score import rouge_scorer\n",
    "# Logging\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity and Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    # testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    # testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "\n",
    "    dataset_name = \"gsm8k\"\n",
    "\n",
    "    if os.path.isdir(f\"./{dataset_name}\"):\n",
    "        print(\"Using Pre-Downloaded Dataset\")\n",
    "        dataset = load_from_disk(\"./gsm8k\")\n",
    "    else:\n",
    "        print(\"Downloading Dataset\")\n",
    "        dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "        print(f\"Saving Dataset to ./{dataset_name}\")\n",
    "        dataset.save_to_disk(\"./gsm8k\")\n",
    "        \n",
    "    print(\"Dataset Loaded\")\n",
    "\n",
    "    testenc = dataset['test']\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc['question']), return_tensors='pt')\n",
    "\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 10\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch, temperature=0.3).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model and Tokenizer\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Model and Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save model and tokenizer to the current directory\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving Model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_pth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:272\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:4331\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m is_from_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4326\u001b[0m     is_safetensors_available()\n\u001b[0;32m   4327\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[0;32m   4329\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4330\u001b[0m ):\n\u001b[1;32m-> 4331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   4332\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[0;32m   4334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4335\u001b[0m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Download Model or Load Model\n",
    "#############################\n",
    "\n",
    "# model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\"\n",
    "model_name = \"wzzju/Qwen2.5-1.5B-GRPO-GSM8K\"\n",
    "model_pth = f\"./{model_name.split('/')[-1]}\"\n",
    "\n",
    "if os.path.isdir(model_pth):\n",
    "    print(\"Using Pre-Downloaded Model and Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_pth, local_files_only=True, padding_side=\"left\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_pth)\n",
    "else:\n",
    "    print(\"Downloading Model and Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Save model and tokenizer to the current directory\n",
    "    print(f\"Saving Model to {model_pth}\")\n",
    "    base_model.save_pretrained(f\"./{model_name.split('/')[-1]}\")\n",
    "    tokenizer.save_pretrained(f\"./{model_name.split('/')[-1]}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "model_perplexity = evaluate(base_model, tokenizer)\n",
    "model_size = get_model_size(base_model, data_width=32, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 8.87\n",
      "model size: 5917.56 MiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Load or Download GSM8k Dataset\n",
    "#################################\n",
    "\n",
    "dataset_name = \"gsm8k\"\n",
    "\n",
    "if os.path.isdir(f\"./{dataset_name}\"):\n",
    "    print(\"Using Pre-Downloaded Dataset\")\n",
    "    dataset = load_from_disk(\"./gsm8k\")\n",
    "else:\n",
    "    print(\"Downloading Dataset\")\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    print(f\"Saving Dataset to ./{dataset_name}\")\n",
    "    dataset.save_to_disk(\"./gsm8k\")\n",
    "    \n",
    "print(\"Dataset Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(f\"./{dataset_name}_tokenized\"):\n",
    "    tokenized_data = load_from_disk(f\"./{dataset_name}_tokenized\")\n",
    "else:\n",
    "    def extract_final_answer(answer):\n",
    "        \"\"\"\n",
    "        Extracts only the numerical value after '####' in the answer field.\n",
    "        \"\"\"\n",
    "        match = re.search(r\"####\\s*([\\d\\.]+)\", answer)  # Match number after ####\n",
    "        return float(match.group(1)) if match else 0  # Return extracted number\n",
    "    \n",
    "    # Process training and test sets\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        dataset[split] = dataset[split].map(lambda example: {\n",
    "            \"original_answer\": example['answer'],\n",
    "            \"question\": example[\"question\"],\n",
    "            # \"answer\": tokenizer(extract_final_answer(example[\"answer\"]),\n",
    "            #                     padding='max_length',\n",
    "            #                     truncation=True,\n",
    "            #                     max_length=16,\n",
    "            #                     return_tensors='pt').to(device),\n",
    "            \"answer\": extract_final_answer(example[\"answer\"]),\n",
    "        })\n",
    "\n",
    "    def format_example(example):\n",
    "        # print(example)\n",
    "        return f\"You are a math expert. Now answer this question - \" + example[\"question\"] + \" Your answer should only contain the final answer as a number. Print final answer here: \"\n",
    "        # return f\"Question: YOU ARE A EXPERT AT MATH. NOW ANSWER THIS QUESTION - {example['question']}. REPLY JUST THE FINAL ANSWER AS A NUMBER. Answer: \"\n",
    "\n",
    "    # Tokenize data\n",
    "    def preprocess_function(examples):\n",
    "        texts = format_example(examples)\n",
    "        tokens = tokenizer(texts, \n",
    "                        padding=\"max_length\", \n",
    "                        truncation=True, \n",
    "                        max_length=128, \n",
    "                        return_tensors=\"pt\")\n",
    "        return tokens\n",
    "\n",
    "    tokenized_data = dataset.map(preprocess_function, batched=False)\n",
    "    # Save processed dataset\n",
    "    tokenized_data.save_to_disk(\"./gsm8k_tokenized\")\n",
    "\n",
    "# Print an example to verify\n",
    "# print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_data = tokenized_data[\"train\"]\n",
    "test_data = tokenized_data[\"test\"]\n",
    "\n",
    "small_train_dataset = train_data.shuffle(seed=42).select(range(1000)) # Loading only 1000\n",
    "small_eval_dataset = test_data.shuffle(seed=42)#.select(range(5))\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_predictions(model, dataloader, device, display=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_training_steps = len(dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    accuracy_log = []\n",
    "    accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            # print(sample)\n",
    "            batch = {}\n",
    "            for k, v in sample.items():\n",
    "                if k != \"question\" and k != \"answer\" and k != 'original_answer':\n",
    "                    batch[k] = torch.tensor(v).to(device)\n",
    "            \n",
    "            output = model.generate(**batch, max_new_tokens=16, do_sample=False)\n",
    "            # if isinstance(output, tuple):  # Ensure proper indexing\n",
    "            #     output = output[0]\n",
    "            \n",
    "            # output = output[len(batch['input_ids']):]\n",
    "            output = tokenizer.decode(output[0][len(batch['input_ids'][0]):], skip_special_tokens=True) \n",
    "\n",
    "            match = re.search(r\"\\s*([\\d\\.]+)\", output)  # Match number after ####\n",
    "            generated_answer = float(match.group(1)) if match else 0  # Return extracted number\n",
    "            \n",
    "            if display:\n",
    "                print(f\"Example {i+1}:\\n\")\n",
    "                print(f\"Input: {sample['question']}\\n\")\n",
    "                print(f\"Generated Answer: {output}\\n\")\n",
    "                print(f\"Target Output: {sample['answer'].item()}\\n\")\n",
    "                print(f\"Output Answer: {generated_answer}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            accuracy = (generated_answer == sample['answer'].item())\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    print(f\"Accuracy: {np.sum(accuracy_log)/len(accuracy_log)}\")\n",
    "    print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18dd147effd4edd9ce14ddaccc8647b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# print_model_predictions(base_model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, sample, device):\n",
    "    batch = {}\n",
    "    for k, v in sample.items():\n",
    "        if k != \"question\" and k != \"answer\" and k != 'original_answer':\n",
    "            batch[k] = torch.tensor(v).to(device)\n",
    "    \n",
    "    output = model.generate(**batch, max_new_tokens=16, do_sample=False)\n",
    "    output = tokenizer.decode(output[0][len(batch['input_ids'][0]):], skip_special_tokens=True) \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_test_accuracy(model, tokenizer, dataloader, device, display=False):\n",
    "    # Make the model eval\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    total = len(dataloader)\n",
    "    num_training_steps = total\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # Evaluate - Basic\n",
    "    accuracy_log = []\n",
    "    accuracy = 0\n",
    "    \n",
    "\n",
    "    # ROUGE Scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    # Open File for Logging\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "    log_file = open(f\"logs/{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\", \"w\")\n",
    "    log_file.write(\"Sample\\tMatch\\tRouge1\\tRouge2\\tRougeL\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "\n",
    "            output = generate_answer(model, tokenizer, sample, device)\n",
    "\n",
    "            match = re.search(r\"\\s*([\\d]+)\", output)  # Match number after ####\n",
    "            generated_answer = float(match.group(1)) if match else 0  # Return extracted number\n",
    "            \n",
    "            accuracy = (generated_answer == sample['answer'].item())\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            # Compute ROUGE scores\n",
    "            scores = scorer.score(sample['original_answer'][0], output)\n",
    "\n",
    "            rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "            rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "            rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "            if display:\n",
    "                print(f\"Example {i+1}:\\n\")\n",
    "                print(f\"Input: {sample['question']}\\n\")\n",
    "                print(f\"Generated Answer: {output}\\n\")\n",
    "                print(f\"Target Output: {sample['answer'].item()}\\n\")\n",
    "                print(f\"Output Answer: {generated_answer}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            log_file.write(f\"{i}\\t{accuracy:.2f}\\t{scores['rouge1'].fmeasure:.4f}\\t{scores['rouge2'].fmeasure:.4f}\\t{scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "            if i % 100 == 0:\n",
    "                print(f\"{i}\\t{accuracy:.2f}\\t{scores['rouge1'].fmeasure:.4f}\\t{scores['rouge2'].fmeasure:.4f}\\t{scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "    accuracy = np.sum(accuracy_log) / total * 100\n",
    "    \n",
    "    # Calculate Average ROUGE Scores\n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "    print(f\"Model Accuracy on GSM8K: {accuracy:.2f}%\")\n",
    "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
    "\n",
    "    log_file.write(f\"Avg\\t{accuracy:.2f}\\t{avg_rouge1:.4f}\\t{avg_rouge2:.4f}\\t{avg_rougeL:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8e3a7b2264657bc3fbaed8ad49204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.1389\t0.0857\t0.1389\n",
      "\n",
      "Model Accuracy on GSM8K: 0.00%\n",
      "Average ROUGE-1: 0.1633\n",
      "Average ROUGE-2: 0.0842\n",
      "Average ROUGE-L: 0.1415\n"
     ]
    }
   ],
   "source": [
    "measure_test_accuracy(base_model, tokenizer, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
