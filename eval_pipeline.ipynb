{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import utils.visulaiser as visulaiser\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from rouge_score import rouge_scorer\n",
    "# Logging\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f62954968d345b2b44ebc65629314c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Download Model or Load Model\n",
    "#############################\n",
    "\n",
    "model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\"\n",
    "model_pth = f\"./{model_name.split('/')[-1]}\"\n",
    "\n",
    "if os.path.isdir(model_pth):\n",
    "    print(\"Using Pre-Downloaded Model and Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_pth, local_files_only=True, padding_side=\"left\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_pth)\n",
    "else:\n",
    "    print(\"Downloading Model and Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Save model and tokenizer to the current directory\n",
    "    print(f\"Saving Model to {model_pth}\")\n",
    "    base_model.save_pretrained(f\"./{model_name.split('/')[-1]}\")\n",
    "    tokenizer.save_pretrained(f\"./{model_name.split('/')[-1]}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Load or Download GSM8k Dataset\n",
    "#################################\n",
    "\n",
    "dataset_name = \"gsm8k\"\n",
    "\n",
    "if os.path.isdir(f\"./{dataset_name}\"):\n",
    "    print(\"Using Pre-Downloaded Dataset\")\n",
    "    dataset = load_from_disk(\"./gsm8k\")\n",
    "else:\n",
    "    print(\"Downloading Dataset\")\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    print(f\"Saving Dataset to ./{dataset_name}\")\n",
    "    dataset.save_to_disk(\"./gsm8k\")\n",
    "    \n",
    "print(\"Dataset Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(f\"./{dataset_name}_tokenized\"):\n",
    "    tokenized_data = load_from_disk(f\"./{dataset_name}_tokenized\")\n",
    "else:\n",
    "    def extract_final_answer(answer):\n",
    "        \"\"\"\n",
    "        Extracts only the numerical value after '####' in the answer field.\n",
    "        \"\"\"\n",
    "        match = re.search(r\"####\\s*([\\d\\.]+)\", answer)  # Match number after ####\n",
    "        return float(match.group(1)) if match else 0  # Return extracted number\n",
    "    \n",
    "    # Process training and test sets\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        dataset[split] = dataset[split].map(lambda example: {\n",
    "            \"original_answer\": example['answer'],\n",
    "            \"question\": example[\"question\"],\n",
    "            # \"answer\": tokenizer(extract_final_answer(example[\"answer\"]),\n",
    "            #                     padding='max_length',\n",
    "            #                     truncation=True,\n",
    "            #                     max_length=16,\n",
    "            #                     return_tensors='pt').to(device),\n",
    "            \"answer\": extract_final_answer(example[\"answer\"]),\n",
    "        })\n",
    "\n",
    "    def format_example(example):\n",
    "        # print(example)\n",
    "        return f\"You are a math expert. Now answer this question - \" + example[\"question\"] + \" Your answer should only contain the final answer as a number. Print final answer here: \"\n",
    "        # return f\"Question: YOU ARE A EXPERT AT MATH. NOW ANSWER THIS QUESTION - {example['question']}. REPLY JUST THE FINAL ANSWER AS A NUMBER. Answer: \"\n",
    "\n",
    "    # Tokenize data\n",
    "    def preprocess_function(examples):\n",
    "        texts = format_example(examples)\n",
    "        tokens = tokenizer(texts, \n",
    "                        padding=\"max_length\", \n",
    "                        truncation=True, \n",
    "                        max_length=128, \n",
    "                        return_tensors=\"pt\")\n",
    "        return tokens\n",
    "\n",
    "    tokenized_data = dataset.map(preprocess_function, batched=False)\n",
    "    # Save processed dataset\n",
    "    tokenized_data.save_to_disk(\"./gsm8k_tokenized\")\n",
    "\n",
    "# Print an example to verify\n",
    "# print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_data = tokenized_data[\"train\"]\n",
    "test_data = tokenized_data[\"test\"]\n",
    "\n",
    "small_train_dataset = train_data.shuffle(seed=42).select(range(1000)) # Loading only 1000\n",
    "small_eval_dataset = test_data.shuffle(seed=42)#.select(range(5))\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_predictions(model, dataloader, device, display=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_training_steps = len(dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    accuracy_log = []\n",
    "    accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            # print(sample)\n",
    "            batch = {}\n",
    "            for k, v in sample.items():\n",
    "                if k != \"question\" and k != \"answer\" and k != 'original_answer':\n",
    "                    batch[k] = torch.tensor(v).to(device)\n",
    "            \n",
    "            output = model.generate(**batch, max_new_tokens=16, do_sample=False)\n",
    "            # if isinstance(output, tuple):  # Ensure proper indexing\n",
    "            #     output = output[0]\n",
    "            \n",
    "            # output = output[len(batch['input_ids']):]\n",
    "            output = tokenizer.decode(output[0][len(batch['input_ids'][0]):], skip_special_tokens=True) \n",
    "\n",
    "            match = re.search(r\"\\s*([\\d\\.]+)\", output)  # Match number after ####\n",
    "            generated_answer = float(match.group(1)) if match else 0  # Return extracted number\n",
    "            \n",
    "            if display:\n",
    "                print(f\"Example {i+1}:\\n\")\n",
    "                print(f\"Input: {sample['question']}\\n\")\n",
    "                print(f\"Generated Answer: {output}\\n\")\n",
    "                print(f\"Target Output: {sample['answer'].item()}\\n\")\n",
    "                print(f\"Output Answer: {generated_answer}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            accuracy = (generated_answer == sample['answer'].item())\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    print(f\"Accuracy: {np.sum(accuracy_log)/len(accuracy_log)}\")\n",
    "    print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18dd147effd4edd9ce14ddaccc8647b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# print_model_predictions(base_model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, sample, device):\n",
    "    batch = {}\n",
    "    for k, v in sample.items():\n",
    "        if k != \"question\" and k != \"answer\" and k != 'original_answer':\n",
    "            batch[k] = torch.tensor(v).to(device)\n",
    "    \n",
    "    output = model.generate(**batch, max_new_tokens=16, do_sample=False)\n",
    "    output = tokenizer.decode(output[0][len(batch['input_ids'][0]):], skip_special_tokens=True) \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_test_accuracy(model, tokenizer, dataloader, device, display=False):\n",
    "    # Make the model eval\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    total = len(dataloader)\n",
    "    num_training_steps = total\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # Evaluate - Basic\n",
    "    accuracy_log = []\n",
    "    accuracy = 0\n",
    "    \n",
    "\n",
    "    # ROUGE Scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    # Open File for Logging\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "    log_file = open(f\"logs/{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\", \"w\")\n",
    "    log_file.write(\"Sample\\tMatch\\tRouge1\\tRouge2\\tRougeL\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "\n",
    "            output = generate_answer(model, tokenizer, sample, device)\n",
    "\n",
    "            match = re.search(r\"\\s*([\\d\\.]+)\", output)  # Match number after ####\n",
    "            generated_answer = float(match.group(1)) if match else 0  # Return extracted number\n",
    "            \n",
    "            accuracy = (generated_answer == sample['answer'].item())\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            # Compute ROUGE scores\n",
    "            scores = scorer.score(sample['original_answer'][0], output)\n",
    "\n",
    "            rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "            rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "            rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "            if display:\n",
    "                print(f\"Example {i+1}:\\n\")\n",
    "                print(f\"Input: {sample['question']}\\n\")\n",
    "                print(f\"Generated Answer: {output}\\n\")\n",
    "                print(f\"Target Output: {sample['answer'].item()}\\n\")\n",
    "                print(f\"Output Answer: {generated_answer}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            log_file.write(f\"{i}\\t{accuracy:.2f}\\t{scores['rouge1'].fmeasure:.4f}\\t{scores['rouge2'].fmeasure:.4f}\\t{scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "            if i % 100 == 0:\n",
    "                print(f\"{i}\\t{accuracy:.2f}\\t{scores['rouge1'].fmeasure:.4f}\\t{scores['rouge2'].fmeasure:.4f}\\t{scores['rougeL'].fmeasure:.4f}\\n\")\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "    accuracy = np.sum(accuracy_log) / total * 100\n",
    "    \n",
    "    # Calculate Average ROUGE Scores\n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "    print(f\"Model Accuracy on GSM8K: {accuracy:.2f}%\")\n",
    "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
    "\n",
    "    log_file.write(f\"Avg\\t{accuracy:.2f}\\t{avg_rouge1:.4f}\\t{avg_rouge2:.4f}\\t{avg_rougeL:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['original_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8e3a7b2264657bc3fbaed8ad49204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.1389\t0.0857\t0.1389\n",
      "\n",
      "Model Accuracy on GSM8K: 0.00%\n",
      "Average ROUGE-1: 0.1633\n",
      "Average ROUGE-2: 0.0842\n",
      "Average ROUGE-L: 0.1415\n"
     ]
    }
   ],
   "source": [
    "measure_test_accuracy(base_model, tokenizer, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
