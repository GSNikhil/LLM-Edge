{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b3f13-a3bf-4b3d-872b-21469c392e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import utils.visulaiser as visulaiser\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555c4fe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5547a353-09ec-4c1b-b2ad-7fb4aded5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b517e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.qwen = base_model.model\n",
    "        self.linear = nn.Linear(896, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.qwen(**inputs)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.linear(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42d910",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "464c81f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1cda6f8b894050896e95b9a623ea48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd20a8c99558432db62e63659d2b730b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 72.0}\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K dataset\n",
    "# dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset = load_from_disk(\"./gsm8k_saved\")\n",
    "\n",
    "def extract_final_answer(answer):\n",
    "    \"\"\"\n",
    "    Extracts only the numerical value after '####' in the answer field.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"####\\s*([\\d\\.]+)\", answer)  # Match number after ####\n",
    "    return float(match.group(1)) if match else 0  # Return extracted number\n",
    "\n",
    "# Process training and test sets\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset[split] = dataset[split].map(lambda example: {\n",
    "        \"question\": example[\"question\"],\n",
    "        # \"answer\": tokenizer(extract_final_answer(example[\"answer\"]),\n",
    "        #                     padding='max_length',\n",
    "        #                     truncation=True,\n",
    "        #                     max_length=16,\n",
    "        #                     return_tensors='pt').to(device),\n",
    "        \"answer\": extract_final_answer(example[\"answer\"])\n",
    "    })\n",
    "\n",
    "# Save processed dataset\n",
    "dataset.save_to_disk(\"./gsm8k_cleaned\")\n",
    "\n",
    "# Print an example to verify\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7d7bced9-cfc5-4e4c-bebe-8b5426d0c094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaac4e6a7e64b5088482d042b13e013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    # print(example)\n",
    "    return f\"Question: YOU ARE A EXPERT AT MATH. NOW ANSWER THIS QUESTION - {example['question']}. REPLY JUST THE FINAL ANSWER AS A NUMBER. Answer: \"\n",
    "\n",
    "# Tokenize data\n",
    "def preprocess_function(examples):\n",
    "    texts = format_example(examples)\n",
    "    tokens = tokenizer(texts, \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True, \n",
    "                     max_length=128, \n",
    "                     return_tensors=\"pt\").to(device)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_data.map(preprocess_function, batched=False)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=False)\n",
    "\n",
    "# Rename \n",
    "tokenized_train = tokenized_train.remove_columns('question')\n",
    "# tokenized_train = tokenized_train.rename_column('answer', 'labels')\n",
    "\n",
    "tokenized_test = tokenized_test.remove_columns('question')\n",
    "# tokenized_test = tokenized_test.rename_column('answer', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "05d88b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train.shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580addba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c8a44",
   "metadata": {},
   "source": [
    "# PyTorch Bare Bones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0735e412b972436daaf3accac7a7fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[242], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_metric(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 31\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     33\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adamw.py:160\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adamw.py:114\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    108\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    109\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m    118\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    119\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    120\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model = ModifiedModel(base_model)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_metric = nn.MSELoss()\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, sample in enumerate(train_dataloader):\n",
    "        batch = {k: torch.tensor(v).to(device) for k, v in sample.items() if k != 'answer'}\n",
    "        \n",
    "        output = model(batch)\n",
    "        if isinstance(output, tuple):  # Ensure proper indexing\n",
    "            output = output[0]\n",
    "        \n",
    "        # Ensure shape consistency for loss calculation\n",
    "        loss = loss_metric(output.view(-1, 1).float(), sample['answer'].view(-1, 1).to(device).float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i % 20 == 0):\n",
    "            print(f\"Step {i}: Loss = {loss.item()}\")\n",
    "\n",
    "    loss_arr.append(running_loss / len(train_dataloader))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb61be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qwen_gsm8k_finetuned\")\n",
    "tokenizer.save_pretrained(\"./qwen_gsm8k_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
