{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "063b3f13-a3bf-4b3d-872b-21469c392e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import utils.visulaiser as visulaiser\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cde6b9-e602-4bfd-8144-ca76c5999371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555c4fe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5547a353-09ec-4c1b-b2ad-7fb4aded5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ba6110e254d8ba1ddc0ba548c9fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gsnik\\.cache\\huggingface\\hub\\models--Qwen--Qwen2-Math-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9bda1d41749f799bae58e7dc53909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94fb60df4104edb9155ed864c1b81e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2cac1cf39b4590a829c7001762a903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdab4aae17684a7fab3cf2bbadae3adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b44125c6224cadb5f369afd9a82f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d22200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Qwen2.5-0.5B-Instruct\\\\tokenizer_config.json',\n",
       " './Qwen2.5-0.5B-Instruct\\\\special_tokens_map.json',\n",
       " './Qwen2.5-0.5B-Instruct\\\\vocab.json',\n",
       " './Qwen2.5-0.5B-Instruct\\\\merges.txt',\n",
       " './Qwen2.5-0.5B-Instruct\\\\added_tokens.json',\n",
       " './Qwen2.5-0.5B-Instruct\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer to the current directory\n",
    "base_model.save_pretrained(\"./Qwen2.5-0.5B-Instruct\")\n",
    "tokenizer.save_pretrained(\"./Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b517e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.qwen = base_model.model\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.qwen(**inputs)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.regression(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42d910",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "464c81f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99571be34fad468db9554f3520e899d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c787ecf285e34c57b9495b270f02a743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 72.0}\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K dataset\n",
    "# dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset = load_from_disk(\"./gsm8k_saved\")\n",
    "\n",
    "def extract_final_answer(answer):\n",
    "    \"\"\"\n",
    "    Extracts only the numerical value after '####' in the answer field.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"####\\s*([\\d\\.]+)\", answer)  # Match number after ####\n",
    "    return float(match.group(1)) if match else 0  # Return extracted number\n",
    "\n",
    "# Process training and test sets\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset[split] = dataset[split].map(lambda example: {\n",
    "        \"question\": example[\"question\"],\n",
    "        # \"answer\": tokenizer(extract_final_answer(example[\"answer\"]),\n",
    "        #                     padding='max_length',\n",
    "        #                     truncation=True,\n",
    "        #                     max_length=16,\n",
    "        #                     return_tensors='pt').to(device),\n",
    "        \"answer\": extract_final_answer(example[\"answer\"])\n",
    "    })\n",
    "\n",
    "# Save processed dataset\n",
    "dataset.save_to_disk(\"./gsm8k_cleaned\")\n",
    "\n",
    "# Print an example to verify\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c2ece33",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    answers.append(train_data[i]['answer'])\n",
    "\n",
    "train_mean = np.mean(answers)\n",
    "train_std = np.std(answers)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i]['answer'] = (train_data[i]['answer'] - train_mean) / train_std\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['answer'] = (test_data[i]['answer'] - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d7bced9-cfc5-4e4c-bebe-8b5426d0c094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b26bfaa738a4909bc61705b2ed94cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    # print(example)\n",
    "    return f\"Question: YOU ARE A EXPERT AT MATH. NOW ANSWER THIS QUESTION - {example['question']}. REPLY JUST THE FINAL ANSWER AS A NUMBER. Answer: \"\n",
    "\n",
    "# Tokenize data\n",
    "def preprocess_function(examples):\n",
    "    texts = format_example(examples)\n",
    "    tokens = tokenizer(texts, \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True, \n",
    "                     max_length=128, \n",
    "                     return_tensors=\"pt\")\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_data.map(preprocess_function, batched=False)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=False)\n",
    "\n",
    "# Rename \n",
    "# tokenized_train = tokenized_train.remove_columns('question')\n",
    "# tokenized_train = tokenized_train.rename_column('answer', 'labels')\n",
    "\n",
    "# tokenized_test = tokenized_test.remove_columns('question')\n",
    "# tokenized_test = tokenized_test.rename_column('answer', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05d88b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train.shuffle(seed=42)#.select(range(1000))\n",
    "small_eval_dataset = tokenized_test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "580addba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbf453",
   "metadata": {},
   "source": [
    "# Explore Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e23ad762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6f07b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_predictions(model, tokenizer, dataset, device=\"cpu\", num_samples=5):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataset = load_from_disk(dataset)\n",
    "    dataset = dataset['test']\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        example = dataset[56+i]\n",
    "        input_text = \"You are a math expert. Now answer this question - \" + example[\"question\"] + \" Your answer should only contain the final answer as a number. Print final answer here: \"\n",
    "        target_output = example[\"answer\"]\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=64)\n",
    "        generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Example {i+1}:\\n\")\n",
    "        print(f\"Input: {input_text}\\n\")\n",
    "        print(f\"Generated Answer: {generated_answer}\\n\")\n",
    "        print(f\"Target Output: {target_output}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "        if i == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486fd397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "\n",
      "Input: You are a math expert. Now answer this question - Peter plans to go to the movies this week. He always gets a ticket for $7 and popcorn for $7. If he has 42 dollars for the week, how many times can he go to the movies? Your answer should only contain the final answer as a number. Print final answer here: \n",
      "\n",
      "Generated Answer: You are a math expert. Now answer this question - Peter plans to go to the movies this week. He always gets a ticket for $7 and popcorn for $7. If he has 42 dollars for the week, how many times can he go to the movies? Your answer should only contain the final answer as a number. Print final answer here: 3\n",
      "To determine how many times Peter can go to the movies, we need to calculate the total cost for each visit and compare it to his weekly budget.\n",
      "\n",
      "1. **Identify the cost per visit:**\n",
      "   - Ticket: $7\n",
      "   - Popcorn: $7\n",
      "   - Total cost per visit\n",
      "\n",
      "Target Output: It costs him $14 to go to the movies because 7 + 7 = <<7+7=14>>14\n",
      "He can go to the movies 3 times because 42 / 14 = <<42/14=3>>3\n",
      "#### 3\n",
      "\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "\n",
      "Input: You are a math expert. Now answer this question - A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty truck is 3755 pounds. What is the maximum number of boxes which can be loaded onto the truck while not exceeding the bridge's weight limit? Your answer should only contain the final answer as a number. Print final answer here: \n",
      "\n",
      "Generated Answer: You are a math expert. Now answer this question - A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty truck is 3755 pounds. What is the maximum number of boxes which can be loaded onto the truck while not exceeding the bridge's weight limit? Your answer should only contain the final answer as a number. Print final answer here: 333\n",
      "To determine the maximum number of boxes that can be loaded onto the truck without exceeding the bridge's weight limit, we need to follow these steps:\n",
      "\n",
      "1. Identify the total weight capacity of the bridge.\n",
      "2. Subtract the weight of the driver and the empty truck from the bridge's weight capacity to find\n",
      "\n",
      "Target Output: The boxes can weigh up to 5000 pounds - 3755 pounds = <<5000-3755=1245>>1245 pounds in total.\n",
      "There can be 1245 / 15 = <<1245/15=83>>83 boxes loaded onto the truck without exceeding the bridge's weight limit.\n",
      "#### 83\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_model_predictions(base_model, tokenizer, \"./gsm8k_saved\", device, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c8a44",
   "metadata": {},
   "source": [
    "# PyTorch Bare Bones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e614e8f-c364-4770-a759-07e03588fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_metric, optimizer, num_epochs):\n",
    "    num_training_steps = num_epochs * len(dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    model = ModifiedModel(base_model)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-3)\n",
    "    loss_metric = nn.MSELoss()\n",
    "    \n",
    "    loss_arr = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            # print(sample)\n",
    "            batch = {k: torch.tensor(v).to(device) for k, v in sample.items() if k != 'answer' and k != 'question'}\n",
    "            \n",
    "            output = model(batch)\n",
    "            if isinstance(output, tuple):  # Ensure proper indexing\n",
    "                output = output[0]\n",
    "            \n",
    "            # Ensure shape consistency for loss calculation\n",
    "            loss = loss_metric(output.view(-1, 1).float(), sample['answer'].view(-1, 1).to(device).float())\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            if (i % 100 == 0):\n",
    "                print(output.item(), sample['answer'].item())\n",
    "                print(f\"Step {i}: Loss = {loss.item()}\")\n",
    "    \n",
    "        loss_arr.append(running_loss / len(train_dataloader))\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return loss_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba46874f-ff79-477b-82dd-3e73d3b1ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedModel(base_model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-3)\n",
    "loss_metric = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cf871de-b29e-4e80-8d31-92ad32768669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38a709dfd4d4474b117fc18930a0644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, loss_metric, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_metric(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     30\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adamw.py:160\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adamw.py:118\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    114\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    115\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    124\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    125\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, loss_metric, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93c20256-70c4-44dc-9ef3-2ad0ed74001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27e643c6-8632-4f50-8881-03ab6bb747e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb61be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), './qwen_gsm8k_finetuned/pytorch_model.bin')\n",
    "tokenizer.save_pretrained(\"./qwen_gsm8k_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cbd69-9122-49ee-a719-1e130d0cd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model again\n",
    "model = ModifiedModel(base_model)  # Recreate your modified model\n",
    "model.load_state_dict(torch.load('./qwen_gsm8k_finetuned/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869a9cf-1239-4a06-ae6f-908e67e4a366",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c195faf-ef79-4525-b6c7-b868306811b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_training_steps = len(dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    running_loss = 0\n",
    "    nCorrect = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            batch = {k: torch.tensor(v).to(device) for k, v in sample.items() if k != 'answer'}\n",
    "            \n",
    "            output = model(batch)\n",
    "            if isinstance(output, tuple):  # Ensure proper indexing\n",
    "                output = output[0]\n",
    "            \n",
    "            # Ensure shape consistency for loss calculation\n",
    "            loss = loss_metric(output.view(-1, 1).float(), sample['answer'].view(-1, 1).to(device).float())\n",
    "            nCorrect += (output.view(-1, 1).float() == sample['answer'].view(-1, 1).to(device).float()) \n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i % 100 == 0):\n",
    "                print(f\"Step {i}: Loss = {loss.item()}\")\n",
    "\n",
    "    return nCorrect / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2876afc5-e2ff-44c5-b7dc-61e4f3fe87de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807a6e3ff95141ac84236b6344f50a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 12708.708984375\n",
      "Step 100: Loss = 899.7828979492188\n",
      "Step 200: Loss = 3.757678508758545\n",
      "Step 300: Loss = 47496.15625\n",
      "Step 400: Loss = 247942.71875\n",
      "Step 500: Loss = 288.5423278808594\n",
      "Step 600: Loss = 104895.5078125\n",
      "Step 700: Loss = 625088448.0\n",
      "Step 800: Loss = 128.3656768798828\n",
      "Step 900: Loss = 2277.062744140625\n",
      "Step 1000: Loss = 15.132217407226562\n",
      "Step 1100: Loss = 658.2655029296875\n",
      "Step 1200: Loss = 487.9842834472656\n",
      "Step 1300: Loss = 668.4796142578125\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edbd77e0-192a-4d14-8da5-e43b44eab608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: tensor([[0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\" , acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
