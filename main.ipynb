{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installing packages...')\n",
    "!pip install transformers accelerate sentencepiece tokenizers datasets tqdm zstandard rouge_score\n",
    "!pip install datasets --upgrade\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# import utils.visulaiser as visulaiser\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from rouge_score import rouge_scorer\n",
    "# Logging\n",
    "from datetime import datetime\n",
    "\n",
    "from download_datasets_models import get_dataset, get_model\n",
    "from evaluate_llm import measure_test_accuracy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Detected Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-Math-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c43b320e5974350afcdf2f62e2629a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model(model_name, save_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"gsm8k\"\n",
    "\n",
    "dataset = get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(f\"./{dataset_name}_tokenized\"):\n",
    "    tokenized_data = load_from_disk(f\"./{dataset_name}_tokenized\")\n",
    "else:\n",
    "    def extract_final_answer(answer):\n",
    "        \"\"\"\n",
    "        Extracts only the numerical value after '####' in the answer field.\n",
    "        \"\"\"\n",
    "        match = re.search(r\"####\\s*([\\d\\.]+)\", answer)  # Match number after ####\n",
    "        return float(match.group(1)) if match else 0  # Return extracted number\n",
    "    \n",
    "    # Process training and test sets\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        dataset[split] = dataset[split].map(lambda example: {\n",
    "            \"original_answer\": example['answer'],\n",
    "            \"question\": example[\"question\"],\n",
    "            # \"answer\": tokenizer(extract_final_answer(example[\"answer\"]),\n",
    "            #                     padding='max_length',\n",
    "            #                     truncation=True,\n",
    "            #                     max_length=16,\n",
    "            #                     return_tensors='pt').to(device),\n",
    "            \"answer\": extract_final_answer(example[\"answer\"]),\n",
    "        })\n",
    "\n",
    "    def format_example(example):\n",
    "        # print(example)\n",
    "        return f\"You are a math expert. Now answer this question - \" + example[\"question\"] + \" Your answer should only contain the final answer as a number. Print final answer here: \"\n",
    "        # return f\"Question: YOU ARE A EXPERT AT MATH. NOW ANSWER THIS QUESTION - {example['question']}. REPLY JUST THE FINAL ANSWER AS A NUMBER. Answer: \"\n",
    "\n",
    "    # Tokenize data\n",
    "    def preprocess_function(examples):\n",
    "        texts = format_example(examples)\n",
    "        tokens = tokenizer(texts, \n",
    "                        padding=\"max_length\", \n",
    "                        truncation=True, \n",
    "                        max_length=128, \n",
    "                        return_tensors=\"pt\")\n",
    "        return tokens\n",
    "\n",
    "    tokenized_data = dataset.map(preprocess_function, batched=False)\n",
    "    # Save processed dataset\n",
    "    tokenized_data.save_to_disk(\"./gsm8k_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "# Commenting Train dataset for now\n",
    "\n",
    "# train_data = tokenized_data[\"train\"]\n",
    "test_data = tokenized_data[\"test\"]\n",
    "\n",
    "# small_train_dataset = train_data.shuffle(seed=42).select(range(1000)) # Loading only 1000\n",
    "small_eval_dataset = test_data.shuffle(seed=42).select(range(200)) # Loading only 200 for quick runs\n",
    "\n",
    "# train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_predictions(model, dataloader, device, num_samples=3, display=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_training_steps = len(dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    accuracy_log = []\n",
    "    accuracy = 0\n",
    "\n",
    "    print(f\"Running only for {num_samples=}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            # print(sample)\n",
    "            batch = {}\n",
    "            for k, v in sample.items():\n",
    "                if k != \"question\" and k != \"answer\" and k != 'original_answer':\n",
    "                    batch[k] = torch.tensor(v).to(device)\n",
    "            \n",
    "            output = model.generate(**batch, max_new_tokens=16, do_sample=False)\n",
    "            # if isinstance(output, tuple):  # Ensure proper indexing\n",
    "            #     output = output[0]\n",
    "            \n",
    "            # output = output[len(batch['input_ids']):]\n",
    "            output = tokenizer.decode(output[0][len(batch['input_ids'][0]):], skip_special_tokens=True) \n",
    "\n",
    "            match = re.search(r\"\\s*([\\d\\.]+)\", output)  # Match number after ####\n",
    "            generated_answer = float(match.group(1)) if match else 0  # Return extracted number\n",
    "            \n",
    "            if display:\n",
    "                print(f\"Example {i+1}:\\n\")\n",
    "                print(f\"Input: {sample['question']}\\n\")\n",
    "                print(f\"Generated Answer: {output}\\n\")\n",
    "                print(f\"Target Output: {sample['answer'].item()}\\n\")\n",
    "                print(f\"Output Answer: {generated_answer}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            accuracy = (generated_answer == sample['answer'].item())\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            if num_samples == i:\n",
    "                break\n",
    "\n",
    "    print(f\"Accuracy: {np.sum(accuracy_log)/len(accuracy_log)}\")\n",
    "    print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_model_predictions(model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq_quantizer import pseudo_quantize_model_weight_scaleup, get_calib_feat\n",
    "from util_functions import get_model_size, evaluate_perplexity\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 1543714304\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of Parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6429873b3a3413483371af471e49671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n",
      "==================================================\n",
      "Base Model\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80865 > 32768). Running this sequence through the model will result in indexing errors\n",
      "evaluating...: 100%|██████████| 10/10 [00:25<00:00,  2.53s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d43a6160d4489ab18f10e0b50a7893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.1389\t0.0857\t0.1389\n",
      "\n",
      "100\t0.00\t0.1714\t0.0000\t0.1714\n",
      "\n",
      "Model Accuracy on GSM8K: 11.00%\n",
      "Average ROUGE-1: 0.1531\n",
      "Average ROUGE-2: 0.0603\n",
      "Average ROUGE-L: 0.1260\n",
      "\n",
      "model perplexity: 8.87\n",
      "model size: 5917.56 MiB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model, tokenizer = get_model()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"=\" * 50)\n",
    "print(\"Base Model\")\n",
    "model = model.to(device)\n",
    "model_perplexity = evaluate_perplexity(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=32, group_size=128)\n",
    "measure_test_accuracy(model, tokenizer, eval_dataloader, device)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size:.2f} MiB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove from GPU Memory\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3a554817444928874377e9411cec08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gsnik\\.cache\\huggingface\\hub\\models--wzzju--Qwen2.5-1.5B-GRPO-GSM8K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model to ./Qwen2.5-1.5B-GRPO-GSM8K\n",
      "Model and Tokenizer Loaded\n",
      "==================================================\n",
      "FineTuned Model\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234d5d4f8c904e5ea0e563e880fb0344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gsnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.0000\t0.0000\t0.0000\n",
      "\n",
      "100\t1.00\t0.0606\t0.0000\t0.0606\n",
      "\n",
      "Model Accuracy on GSM8K: 4.00%\n",
      "Average ROUGE-1: 0.0429\n",
      "Average ROUGE-2: 0.0087\n",
      "Average ROUGE-L: 0.0379\n",
      "\n",
      "model perplexity: 9.34\n",
      "model size: 5917.56 MiB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# FineTuned Model\n",
    "fmodel, ftokenizer = get_model(\"wzzju/Qwen2.5-1.5B-GRPO-GSM8K\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"=\" * 50)\n",
    "print(\"FineTuned Model\")\n",
    "fmodel = fmodel.to(device)\n",
    "model_perplexity = evaluate_perplexity(fmodel, ftokenizer)\n",
    "model_size = get_model_size(fmodel, data_width=32, group_size=128)\n",
    "measure_test_accuracy(fmodel, ftokenizer, eval_dataloader, device)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size:.2f} MiB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove from GPU Memory\n",
    "del fmodel\n",
    "del ftokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f90f899c0024460b96a82356eee8492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n",
      "Collecting activation scales...\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n",
      " * Split into 30 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e42349c57e44f8aa198cf4627f38799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n",
      "==================================================\n",
      "scale_factor=4, bit=2\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80865 > 32768). Running this sequence through the model will result in indexing errors\n",
      "evaluating...: 100%|██████████| 10/10 [00:27<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06252ea3370430e830afca7bda067bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.0290\t0.0000\t0.0290\n",
      "\n",
      "100\t0.00\t0.0000\t0.0000\t0.0000\n",
      "\n",
      "Model Accuracy on GSM8K: 0.00%\n",
      "Average ROUGE-1: 0.0128\n",
      "Average ROUGE-2: 0.0003\n",
      "Average ROUGE-L: 0.0123\n",
      "\n",
      "model perplexity: 96943.91\n",
      "model size: 396.80 MiB\n",
      "==================================================\n",
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a09b46ad0242399a54bfe9ebd5bd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n",
      "==================================================\n",
      "scale_factor=4, bit=4\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80865 > 32768). Running this sequence through the model will result in indexing errors\n",
      "evaluating...: 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f42faa99ba04095b4f961f47f2580b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.1081\t0.0556\t0.1081\n",
      "\n",
      "100\t0.00\t0.2286\t0.0000\t0.2286\n",
      "\n",
      "Model Accuracy on GSM8K: 4.50%\n",
      "Average ROUGE-1: 0.1491\n",
      "Average ROUGE-2: 0.0577\n",
      "Average ROUGE-L: 0.1235\n",
      "\n",
      "model perplexity: 9.93\n",
      "model size: 764.85 MiB\n",
      "==================================================\n",
      "Using Pre-Downloaded Model and Tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bab4a7870c345de8a2507fbaa0c8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded\n",
      "==================================================\n",
      "scale_factor=4, bit=8\n",
      "Using Pre-Downloaded Dataset\n",
      "Dataset Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80865 > 32768). Running this sequence through the model will result in indexing errors\n",
      "evaluating...: 100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24956e5cc7a4b75bb2219b09b0fb006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.00\t0.1389\t0.0857\t0.1389\n",
      "\n",
      "100\t0.00\t0.1714\t0.0000\t0.1714\n",
      "\n",
      "Model Accuracy on GSM8K: 10.50%\n",
      "Average ROUGE-1: 0.1546\n",
      "Average ROUGE-2: 0.0604\n",
      "Average ROUGE-L: 0.1267\n",
      "\n",
      "model perplexity: 8.87\n",
      "model size: 1500.95 MiB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model()\n",
    "model = model.to(device)\n",
    "input_feat = get_calib_feat(model, tokenizer)\n",
    "\n",
    "for scale_factor in [4]:\n",
    "    for bit in [2, 4, 8]:\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model, tokenizer = get_model()\n",
    "        pseudo_quantize_model_weight_scaleup(model, w_bit=bit, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model = model.to(device)\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{scale_factor=}, {bit=}\")\n",
    "        model_perplexity = evaluate_perplexity(model, tokenizer)\n",
    "        model_size = get_model_size(model, data_width=bit, group_size=128)\n",
    "        measure_test_accuracy(model, tokenizer, eval_dataloader, device)\n",
    "        print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "        print(f\"model size: {model_size:.2f} MiB\")\n",
    "\n",
    "        # model.save_pretrained(f\"./{model_name.split('/')[-1]}_{bit}bit\")\n",
    "        # tokenizer.save_pretrained(f\"./{model_name.split('/')[-1]}_{bit}bit\")\n",
    "        print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
